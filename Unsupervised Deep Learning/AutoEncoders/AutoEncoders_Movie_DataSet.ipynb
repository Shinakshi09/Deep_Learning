{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoders_Movie_DataSet.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tkIiVQ4oQa4",
        "colab_type": "text"
      },
      "source": [
        "# Note :\n",
        "\n",
        "\n",
        "```\n",
        "I am using the same dataset which i used in other unsupervised Deep learning \n",
        "\n",
        "algorithm Boltzmann machine hence Data preprocessing steps are same\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85XOebvHctMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Importing the libraries \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.parallel\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.utils.data\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJwAndmdeAqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "importing the Dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "movies = pd.read_csv(\"/content/drive/My Drive/SelfPractise/Boltzmann Machine/ml-1m/movies.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv(\"/content/drive/My Drive/SelfPractise/Boltzmann Machine/ml-1m/ratings.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv(\"/content/drive/My Drive/SelfPractise/Boltzmann Machine/ml-1m/users.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "movies.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0OV1riYg0wO",
        "colab_type": "text"
      },
      "source": [
        "# Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbrdgGpJez8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\" \n",
        "Preparing Training and Test Set \n",
        "NOte : Separater for this file is Tab hence using \\t parameter\n",
        "I am converting these dataset into numpy array because I am using PyTorch Tensors\n",
        "\"\"\"\n",
        "\n",
        "training_set = pd.read_csv(\"/content/drive/My Drive/SelfPractise/Boltzmann Machine/ml-100k/u1.base\", delimiter = '\\t')\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = pd.read_csv(\"/content/drive/My Drive/SelfPractise/Boltzmann Machine/ml-100k/u1.test\", delimiter = '\\t')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QnIJgY6hTRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "Getting the total number of users and Movies, Why Am I doing this?\n",
        "\n",
        "Because I will be converting my training_Set and test_Set into a matrix where lines are going to user and columns are \n",
        "going to be movies and cells are going to be the ratings.\n",
        "\n",
        "I will create such matrix for training set and test set both\n",
        "\n",
        "I will also include older users, older movies from original dataset into these two Matrixes \n",
        "\n",
        "And The Training set that we just imported, If a user did not rate the movies, We will put it a zero\n",
        "into the cell of matrix that corresponds to  this user and that movie\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
        "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKYpghAVhoCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "Going to convert our dataset into an array with users in lines and movies in columns\n",
        "\n",
        "Why Am i doing this?\n",
        "\n",
        "We will need to make a specific structure that will corresponds to  what the Restricted Boltzmann machine wants as input\n",
        "\n",
        "These observations will go one by one into the RBM. Hence will create a structure that will contain these observations\n",
        "that will go into the Network and their different features that are going to be into input nodes and that is what\n",
        "i will do by creating this array with users in lines and movies in the columns because we will have observations in lines and \n",
        "features in the columns. \n",
        "\n",
        "In other words, It is the same like we do in all neural networks or ML ALgorithms.\n",
        "\n",
        "I will accomplish this objective by creating a user defined function. I will give only one argument to this function\n",
        "because it will apply to only one set of data which will training set first and when we will apply this to our training set \n",
        "\n",
        "or test set, We only need to replace this argument by training set and test set, Then colon :\n",
        "\n",
        "after than I will tell the function what it needs to do\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def convert(data):\n",
        "  new_data = []\n",
        "  for id_users in range(1, nb_users + 1):\n",
        "    id_movies = data[:, 1] [data[:, 0] == id_users]\n",
        "    id_ratings = data[:, 2] [data[:, 0] == id_users]\n",
        "    ratings = np.zeros(nb_movies)\n",
        "    ratings[id_movies - 1] = id_ratings\n",
        "    new_data.append(list(ratings))\n",
        "  return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFGjY0DOhyuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\" Converting the data into torch tensor \"\"\"\n",
        "\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3baMyIZnOOH",
        "colab_type": "text"
      },
      "source": [
        "# Architecture of the Autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SK8Zw1riA_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "This will be stacked auto encoder hence neurons of hidden layers are stacked \n",
        "\n",
        "First Fully connected layer will accept the input vector and number of neurons\n",
        "\n",
        "second fully connected layer will accept the number of nuerons from previous hidden layer and number of neurons we want to\n",
        "\n",
        "assign at this layer\n",
        "\n",
        "I am not making this architecture very dense. But Check out my experiment repo. where I have performed so many experiments\n",
        "\n",
        "with dense autoencoder architectures and lightweight also.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-mzwhM9oCke",
        "colab_type": "text"
      },
      "source": [
        "# Training the Autoencoder: \n",
        "\n",
        "SAE = Stacked Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1WDGIAen6tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  train_loss = 0\n",
        "  s = 0.\n",
        "  for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = input.clone()\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "      output = sae(input)\n",
        "      target.require_grad = False\n",
        "      output[target == 0] = 0\n",
        "      loss = criterion(output, target)\n",
        "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "      loss.backward()\n",
        "      train_loss += np.sqrt(loss.data*mean_corrector)\n",
        "      s += 1.\n",
        "      optimizer.step()\n",
        "  print('epoch: '+str(epoch)+'loss: '+ str(train_loss/s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxFV43zYo01D",
        "colab_type": "text"
      },
      "source": [
        "# Testing the Autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKj8UvWQoKza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "  input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "  target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "  if torch.sum(target.data > 0) > 0:\n",
        "    output = sae(input)\n",
        "    target.require_grad = False\n",
        "    output[target == 0] = 0\n",
        "    loss = criterion(output, target)\n",
        "    mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "    test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "    s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}